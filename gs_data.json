{"container_type": "Author", "filled": ["basics", "publications", "indices", "counts"], "scholar_id": "pHPTHHwAAAAJ", "source": "AUTHOR_PROFILE_PAGE", "name": "Xin Sky Li", "url_picture": "https://scholar.googleusercontent.com/citations?view_op=view_photo&user=pHPTHHwAAAAJ&citpid=5", "affiliation": "Beijing University of Posts and Telecommunications", "organization": 8426264526679338639, "interests": ["Large Language Model", "Autonomous Agents", "Reinforcement Learning"], "email_domain": "@bupt.edu.cn", "homepage": "https://lixin4sky.github.io/", "citedby": 50, "publications": {"pHPTHHwAAAAJ:u5HHmVD_uO8C": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Scalable autoregressive image generation with mamba", "pub_year": 2024, "citation": "arXiv preprint arXiv:2408.12245, 2024", "author": "Haopeng Li and Jinyue Yang and Kexin Wang and Xuerui Qiu and Yuhong Chou and Xin Li and Guoqi Li", "journal": "arXiv preprint arXiv:2408.12245", "abstract": "We introduce AiM, an autoregressive (AR) image generative model based on Mamba architecture. AiM employs Mamba, a novel state-space model characterized by its exceptional performance for long-sequence modeling with linear time complexity, to supplant the commonly utilized Transformers in AR image generation models, aiming to achieve both superior generation quality and enhanced inference speed. Unlike existing methods that adapt Mamba to handle two-dimensional signals via multi-directional scan, AiM directly utilizes the next-token prediction paradigm for autoregressive image generation. This approach circumvents the need for extensive modifications to enable Mamba to learn 2D spatial representations. By implementing straightforward yet strategically targeted modifications for visual generative tasks, we preserve Mamba's core structure, fully exploiting its efficient long-sequence modeling capabilities and scalability. We provide AiM models in various scales, with parameter counts ranging from 148M to 1.3B. On the ImageNet1K 256*256 benchmark, our best AiM model achieves a FID of 2.21, surpassing all existing AR models of comparable parameter counts and demonstrating significant competitiveness against diffusion models, with 2 to 10 times faster inference speed. Code is available at https://github.com/hp-l33/AiM"}, "filled": true, "author_pub_id": "pHPTHHwAAAAJ:u5HHmVD_uO8C", "num_citations": 22, "citedby_url": "/scholar?hl=en&cites=16466771172118969587", "cites_id": ["16466771172118969587"], "pub_url": "https://arxiv.org/abs/2408.12245", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:8zzL5yK5heQJ:scholar.google.com/", "cites_per_year": {"2024": 6, "2025": 16}}, "pHPTHHwAAAAJ:u-x6o8ySG0sC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Can Large Language Models Analyze Graphs like Professionals? A Benchmark, Datasets and Models", "pub_year": 2024, "citation": "NeurIPS DB Track 2024, 2024", "author": "Xin Li and Weize Chen and Qizhi Chu and Haopeng Li and Zhaojun Sun and Ran Li and Chen Qian and Yiwei Wei and Zhiyuan Liu and Chuan Shi and Maosong Sun and Cheng Yang", "conference": "NeurIPS DB Track 2024", "abstract": "The need to analyze graphs is ubiquitous across various fields, from social networks to biological research and recommendation systems. Therefore, enabling the ability of large language models (LLMs) to process graphs is an important step toward more advanced general intelligence. However, current LLM benchmarks on graph analysis require models to directly reason over the prompts describing graphtopology, and are thus limited to small graphs with only a few dozens of nodes. In contrast, human experts typically write programs based on popular libraries for task solving, and can thus handle graphs with different scales. To this end, a question naturally arises: can LLMs analyze graphs like professionals? In this paper, we introduce ProGraph, a manually crafted benchmark containing 3 categories of graph tasks. The benchmark expects solutions based on programming instead of directly reasoning over raw inputs. Our findings reveal that the performance of current LLMs is unsatisfactory, with the best model achieving only 36% accuracy. To bridge this gap, we propose LLM4Graph datasets, which include crawled documents and auto-generated codes based on 6 widely used graph libraries. By augmenting closed-source LLMs with document retrieval and fine-tuning open-source ones on the codes, we show 11-32% absolute improvements in their accuracies. Our results underscore that the capabilities of LLMs in handling structured data are still under-explored, and show the effectiveness of LLM4Graph in enhancing LLMs’ proficiency of graph analysis. The benchmark, datasets and enhanced open-sourcemodels are available at https …"}, "filled": true, "author_pub_id": "pHPTHHwAAAAJ:u-x6o8ySG0sC", "num_citations": 17, "citedby_url": "/scholar?hl=en&cites=15809926069109984052", "cites_id": ["15809926069109984052"], "pub_url": "https://proceedings.neurips.cc/paper_files/paper/2024/hash/ff417c3993894694e88ffc4d3f53d28b-Abstract-Datasets_and_Benchmarks_Track.html", "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:NAt9yv4jaNsJ:scholar.google.com/", "cites_per_year": {"2024": 2, "2025": 15}}, "pHPTHHwAAAAJ:d1gkVwhDpl0C": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Graphteam: Facilitating large language model-based graph analysis via multi-agent collaboration", "pub_year": "2024", "citation": "arXiv preprint arXiv:2410.18032, 2024"}, "filled": false, "author_pub_id": "pHPTHHwAAAAJ:d1gkVwhDpl0C", "num_citations": 11, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=9145904115587407543", "cites_id": ["9145904115587407543"]}}, "citedby5y": 50, "hindex": 3, "hindex5y": 3, "i10index": 3, "i10index5y": 3, "cites_per_year": {"2024": 8, "2025": 42}, "updated": "2025-10-30 19:01:37.087854"}